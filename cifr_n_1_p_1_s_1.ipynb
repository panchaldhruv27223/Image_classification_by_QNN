{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Grayscale, Normalize, Compose\n",
    "from torchvision.utils import make_grid\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for preprocessing\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),  # Convert image to PyTorch tensor\n",
    "    Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuanvolutionalLayer(nn.Module):\n",
    "#     def __init__(self, filter_size=2, n_layers=1, stride=2, padding=0):\n",
    "#         \"\"\"\n",
    "#         Quanvolutional Layer in PyTorch\n",
    "#         - filter_size: Size of the quantum filter (e.g., 2x2)\n",
    "#         - n_layers: Number of random quantum layers\n",
    "#         - stride: Stride for the filter movement\n",
    "#         - padding: Padding around the image (not implemented)\n",
    "#         \"\"\"\n",
    "#         super(QuanvolutionalLayer, self).__init__()\n",
    "#         self.filter_size = filter_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.stride = stride\n",
    "#         self.padding = padding\n",
    "\n",
    "#         # Define a quantum device with the required number of qubits (equal to filter_size^2)\n",
    "#         self.dev = qml.device(\"default.qubit\", wires=filter_size**2)\n",
    "\n",
    "#         # Generate random quantum circuit parameters\n",
    "#         self.rand_params = np.random.uniform(0, 2*np.pi, size=(n_layers, filter_size**2))\n",
    "\n",
    "#     def quantum_circuit(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Quantum circuit for the quanvolutional layer.\n",
    "#         This function encodes the inputs into quantum states, applies quantum gates,\n",
    "#         and returns expectation values of Pauli-Z measurements.\n",
    "#         \"\"\"\n",
    "#         @qml.qnode(self.dev, interface=\"torch\")\n",
    "#         def circuit(inputs):\n",
    "#             # Encode classical inputs as quantum rotations\n",
    "#             for i in range(self.filter_size**2):\n",
    "#                 qml.RY(inputs[i] * np.pi, wires=i)\n",
    "\n",
    "#             # Apply random quantum layers\n",
    "#             qml.templates.RandomLayers(self.rand_params, wires=list(range(self.filter_size**2)))\n",
    "\n",
    "#             # Measure expectation values of Pauli-Z\n",
    "#             return [qml.expval(qml.PauliZ(i)) for i in range(self.filter_size**2)]\n",
    "\n",
    "#         return torch.tensor(circuit(inputs), dtype=torch.float32).to(inputs.device)  # Move to correct device\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Applies the quantum transformation to the input image.\n",
    "#         - x: Input image of shape [batch_size, 1, H, W]\n",
    "#         Returns a transformed feature map.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         batch_size, channels, height, width = x.shape\n",
    "        \n",
    "#         if self.padding != 0:\n",
    "#             # Apply padding to input image\n",
    "#             x = torch.nn.functional.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
    "\n",
    "#         # Compute output size using the correct formula\n",
    "#         output_size = ((height + 2 * self.padding - self.filter_size) // self.stride) + 1\n",
    "#         print(f\"generated output size is : {output_size}\")\n",
    "#         # Create an empty tensor to store the output\n",
    "#         output = torch.zeros(batch_size, self.filter_size**2, output_size, output_size)\n",
    "\n",
    "#         # Apply quantum filters to patches\n",
    "#         for i in range(output_size):\n",
    "#             for j in range(output_size):\n",
    "#                 row_start = i * self.stride\n",
    "#                 col_start = j * self.stride\n",
    "#                 patch = x[:, :, row_start:row_start+self.filter_size, col_start:col_start+self.filter_size]\n",
    "#                 patch = patch.contiguous().reshape(batch_size, -1)  # Ensure contiguous memory\n",
    "\n",
    "#                 # Apply quantum circuit on each patch and store results\n",
    "#                 q_results = torch.stack([self.quantum_circuit(p) for p in patch])\n",
    "\n",
    "#                 output[:, :, i, j] = q_results  # Store at corrected index\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "class QuanvolutionalLayer(nn.Module):\n",
    "    def __init__(self, filter_size=2, n_layers=1, stride=2, padding=0):\n",
    "        super(QuanvolutionalLayer, self).__init__()\n",
    "        self.filter_size = filter_size\n",
    "        self.n_layers = n_layers\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Define a quantum device\n",
    "        self.dev = qml.device(\"default.qubit\", wires=filter_size**2)\n",
    "\n",
    "        # Generate random quantum circuit parameters\n",
    "        self.rand_params = np.random.uniform(0, 2*np.pi, size=(n_layers, filter_size**2))\n",
    "\n",
    "    def quantum_circuit(self, inputs):\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def circuit(inputs):\n",
    "            for i in range(self.filter_size**2):\n",
    "                qml.RY(inputs[i] * np.pi, wires=i)\n",
    "\n",
    "            qml.templates.RandomLayers(self.rand_params, wires=list(range(self.filter_size**2)))\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.filter_size**2)]\n",
    "\n",
    "        return torch.tensor(circuit(inputs), dtype=torch.float32).to(inputs.device)  # Move to correct device\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.shape\n",
    "\n",
    "        if self.padding != 0:\n",
    "            x = torch.nn.functional.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
    "\n",
    "        output_size = ((height + 2 * self.padding - self.filter_size) // self.stride) + 1\n",
    "        output = torch.zeros(batch_size, self.filter_size**2, output_size, output_size, device=x.device)  # Move to same device\n",
    "\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                row_start = i * self.stride\n",
    "                col_start = j * self.stride\n",
    "                patch = x[:, :, row_start:row_start+self.filter_size, col_start:col_start+self.filter_size]\n",
    "                patch = patch.contiguous().reshape(batch_size, -1)  \n",
    "\n",
    "                q_results = torch.stack([self.quantum_circuit(p) for p in patch])\n",
    "\n",
    "                output[:, :, i, j] = q_results  # Store at corrected index\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quanvolutional_Convolutional_NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Quanvolutional_Convolutional_NeuralNetwork, self).__init__()\n",
    "        self.quanv1 = QuanvolutionalLayer(filter_size=3, n_layers=1, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=9, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  \n",
    "        self.fc2 = nn.Linear(128, 10)  # Final classification (CIFAR-10 has 10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quanv1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)  \n",
    "        x = self.conv2(x)           \n",
    "        x = F.relu(x)  \n",
    "        x = self.pool(x)          \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)  \n",
    "        x = self.fc2(x) \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Quanvolutional_Convolutional_NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epochs=30):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU if available\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)  # Get class with highest probability\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # Print epoch stats\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {total_loss/len(train_loader):.4f} | Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation needed\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader):.4f} | Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/selc-a4-sr2/myenv/lib/python3.12/site-packages/pennylane/math/interface_utils.py:127: UserWarning: Contains tensors of types {'torch', 'autograd'}; dispatch will prioritize TensorFlow, PyTorch, and Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n",
      "/home/selc-a4-sr2/myenv/lib/python3.12/site-packages/pennylane/math/interface_utils.py:127: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow, PyTorch, and Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model.to(device)  # Move model to GPU\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "test(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"Models\"\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = f\"QC__model_cifr_n_1_p_1_s_1.pth\"\n",
    "final_model_path = os.path.join(model_path, final_model)\n",
    "final_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), final_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
