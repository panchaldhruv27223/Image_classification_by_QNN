{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Grayscale, Normalize, Compose\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for preprocessing\n",
    "transform = Compose([\n",
    "    Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    ToTensor(),  # Convert image to PyTorch tensor\n",
    "    Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pennylane in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (0.40.0)\n",
      "Requirement already satisfied: numpy<2.1 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (3.4.2)\n",
      "Requirement already satisfied: rustworkx>=0.14.0 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (0.16.0)\n",
      "Requirement already satisfied: autograd in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (1.7.0)\n",
      "Requirement already satisfied: tomlkit in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (0.13.2)\n",
      "Requirement already satisfied: appdirs in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: autoray>=0.6.11 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (0.7.0)\n",
      "Requirement already satisfied: cachetools in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (5.5.1)\n",
      "Requirement already satisfied: pennylane-lightning>=0.40 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (0.40.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (4.12.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (24.2)\n",
      "Requirement already satisfied: diastatic-malt in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane) (2.15.2)\n",
      "Requirement already satisfied: scipy-openblas32>=0.3.26 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from pennylane-lightning>=0.40->pennylane) (0.3.29.0.0)\n",
      "Requirement already satisfied: astunparse in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from diastatic-malt->pennylane) (1.6.3)\n",
      "Requirement already satisfied: gast in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from diastatic-malt->pennylane) (0.6.0)\n",
      "Requirement already satisfied: termcolor in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from diastatic-malt->pennylane) (2.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from requests->pennylane) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from requests->pennylane) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from requests->pennylane) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from astunparse->diastatic-malt->pennylane) (0.44.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in c:\\users\\dhruv\\python_envs\\dl_env\\lib\\site-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuanvolutionalLayer(nn.Module):\n",
    "    def __init__(self, filter_size=2, n_layers=1, stride=2, padding=0):\n",
    "        \"\"\"\n",
    "        Quanvolutional Layer in PyTorch\n",
    "        - filter_size: Size of the quantum filter (e.g., 2x2)\n",
    "        - n_layers: Number of random quantum layers\n",
    "        - stride: Stride for the filter movement\n",
    "        - padding: Padding around the image (not implemented)\n",
    "        \"\"\"\n",
    "        super(QuanvolutionalLayer, self).__init__()\n",
    "        self.filter_size = filter_size\n",
    "        self.n_layers = n_layers\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Define a quantum device with the required number of qubits (equal to filter_size^2)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=filter_size**2)\n",
    "\n",
    "        # Generate random quantum circuit parameters\n",
    "        self.rand_params = np.random.uniform(0, 2*np.pi, size=(n_layers, filter_size**2))\n",
    "\n",
    "    def quantum_circuit(self, inputs):\n",
    "        \"\"\"\n",
    "        Quantum circuit for the quanvolutional layer.\n",
    "        This function encodes the inputs into quantum states, applies quantum gates,\n",
    "        and returns expectation values of Pauli-Z measurements.\n",
    "        \"\"\"\n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def circuit(inputs):\n",
    "            # Encode classical inputs as quantum rotations\n",
    "            for i in range(self.filter_size**2):\n",
    "                qml.RY(inputs[i] * np.pi, wires=i)\n",
    "\n",
    "            # Apply random quantum layers\n",
    "            qml.templates.RandomLayers(self.rand_params, wires=list(range(self.filter_size**2)))\n",
    "\n",
    "            # Measure expectation values of Pauli-Z\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.filter_size**2)]\n",
    "\n",
    "        return torch.tensor(circuit(inputs), dtype=torch.float32)  # Convert list to Tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the quantum transformation to the input image.\n",
    "        - x: Input image of shape [batch_size, 1, H, W]\n",
    "        Returns a transformed feature map.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        if self.padding != 0:\n",
    "            # Apply padding to input image\n",
    "            x = torch.nn.functional.pad(x, (self.padding, self.padding, self.padding, self.padding), mode='constant', value=0)\n",
    "\n",
    "        # Compute output size using the correct formula\n",
    "        output_size = ((height + 2 * self.padding - self.filter_size) // self.stride) + 1\n",
    "        print(f\"generated output size is : {output_size}\")\n",
    "        # Create an empty tensor to store the output\n",
    "        output = torch.zeros(batch_size, self.filter_size**2, output_size, output_size)\n",
    "\n",
    "        # Apply quantum filters to patches\n",
    "        for i in range(output_size):\n",
    "            for j in range(output_size):\n",
    "                row_start = i * self.stride\n",
    "                col_start = j * self.stride\n",
    "                patch = x[:, :, row_start:row_start+self.filter_size, col_start:col_start+self.filter_size]\n",
    "                patch = patch.contiguous().reshape(batch_size, -1)  # Ensure contiguous memory\n",
    "\n",
    "                # Apply quantum circuit on each patch and store results\n",
    "                q_results = torch.stack([self.quantum_circuit(p) for p in patch])\n",
    "\n",
    "                output[:, :, i, j] = q_results  # Store at corrected index\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated output size is : 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\Python_Envs\\dl_env\\lib\\site-packages\\pennylane\\math\\interface_utils.py:127: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow, PyTorch, and Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 32, 32])\n",
      "Output shape: torch.Size([4, 9, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# this code is used for just test the working of above function \n",
    "\n",
    "# Create a dummy batch of images [batch_size, channels, height, width]\n",
    "dummy_images = torch.rand(4, 1, 32, 32)  # 4 grayscale images of size 28 X 28\n",
    "\n",
    "# Instantiate the quanvolutional layer\n",
    "quanv = QuanvolutionalLayer(filter_size=3, n_layers=2, stride=1, padding=1)\n",
    "\n",
    "# Apply the quanvolutional transformation\n",
    "output = quanv(dummy_images)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Input shape:\", dummy_images.shape)    # Expected: [4, 1, 32, 32]\n",
    "print(\"Output shape:\", output.shape)         # Expected: [4, 4, 16, 16] (since 2x2 filter → 4 output channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated output size is : 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\Python_Envs\\dl_env\\lib\\site-packages\\pennylane\\math\\interface_utils.py:127: UserWarning: Contains tensors of types {'autograd', 'torch'}; dispatch will prioritize TensorFlow, PyTorch, and Jax over Autograd. Consider replacing Autograd with vanilla NumPy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 32, 32])\n",
      "Output shape: torch.Size([4, 9, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# this code is used for just test the working of above function \n",
    "\n",
    "# Create a dummy batch of images [batch_size, channels, height, width]\n",
    "dummy_images = torch.rand(4, 1, 32, 32)  # 4 grayscale images of size 28 X 28\n",
    "\n",
    "# Instantiate the quanvolutional layer\n",
    "quanv = QuanvolutionalLayer(filter_size=3, n_layers=1, stride=2, padding=1)\n",
    "\n",
    "# Apply the quanvolutional transformation\n",
    "output = quanv(dummy_images)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Input shape:\", dummy_images.shape)    # Expected: [4, 1, 32, 32]\n",
    "print(\"Output shape:\", output.shape)         # Expected: [4, 4, 16, 16] (since 2x2 filter → 4 output channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 32, 32])\n",
      "Output shape: torch.Size([4, 4, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "## this code is used for just test the working of above function \n",
    "\n",
    "# # Create a dummy batch of images [batch_size, channels, height, width]\n",
    "# dummy_images = torch.rand(4, 1, 32, 32)  # 4 grayscale images of size 32x32\n",
    "\n",
    "# # Instantiate the quanvolutional layer\n",
    "# quanv = QuanvolutionalLayer(filter_size=2, n_layers=1, stride=2)\n",
    "\n",
    "# # Apply the quanvolutional transformation\n",
    "# output = quanv(dummy_images)\n",
    "\n",
    "# # Print the shape of the output\n",
    "# print(\"Input shape:\", dummy_images.shape)    # Expected: [4, 1, 32, 32]\n",
    "# print(\"Output shape:\", output.shape)         # Expected: [4, 4, 16, 16] (since 2x2 filter → 4 output channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quanvolutional_Convolutional_NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Quanvolutional_Convolutional_NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.quanv1 = QuanvolutionalLayer(filter_size=3, n_layers=1, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=9, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Output size after pooling\n",
    "        self.fc2 = nn.Linear(128, 10)  # Final classification (CIFAR-10 has 10 classes)\n",
    "\n",
    "        # # Fully connected layer for classification\n",
    "        # self.fc = nn.Linear(4 * 16 * 16, 10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "\n",
    "        # # 1️⃣ Quanvolutional Layer (Extract quantum features)\n",
    "        # self.quanv = QuanvolutionalLayer(filter_size=2, n_layers=1, stride=2)\n",
    "\n",
    "        # 2️⃣ Convolutional Layers (Classic CNN model)\n",
    "        # self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # 3️⃣ Pooling Layer\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 4️⃣ Fully Connected Layers (Dense Layers)\n",
    "        # self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Output size after pooling\n",
    "        # self.fc2 = nn.Linear(128, 10)  # Final classification (CIFAR-10 has 10 classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply quanvolutional layer\n",
    "        x = self.quanv1(x)\n",
    "\n",
    "        # 2️⃣ Pass through CNN layers\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu()  # Conv layer 1 + ReLU\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)           # Apply max pooling\n",
    "        x = F.relu(x)  # Conv layer 2 + ReLU\n",
    "        x = self.pool(x)           # Apply max pooling again\n",
    "\n",
    "        # 3️⃣ Flatten output\n",
    "        x = x.view(x.shape[0], -1)  \n",
    "\n",
    "        # 4️⃣ Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)  \n",
    "        x = self.fc2(x)  # No activation here, since we'll apply softmax in loss function\n",
    "\n",
    "        return x  # Raw \n",
    "\n",
    "        # # Flatten feature maps\n",
    "        # x = x.view(x.shape[0], -1)  \n",
    "\n",
    "        # # Apply fully connected layer\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        # return F.log_softmax(x, dim=1)  # Use log-softmax for numerical stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy batch of grayscale CIFAR-10 images\n",
    "dummy_images = torch.rand(4, 1, 32, 32)  # Batch size: 4, Channels: 1, Height: 32, Width: 32\n",
    "\n",
    "# Instantiate the model\n",
    "model = QuanvolutionalNeuralNetwork()\n",
    "\n",
    "# Forward pass\n",
    "output = model(dummy_images)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Model output shape:\", output.shape)  # Expected: [4, 10] (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.NLLLoss()  # Negative Log Likelihood Loss (for log-softmax output)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epochs=30):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU if available\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)  # Get class with highest probability\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # Print epoch stats\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {total_loss/len(train_loader):.4f} | Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradient calculation needed\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader):.4f} | Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model.to(device)  # Move model to GPU\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "test(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
